summary: Logs deep dive (2022)
id: log2022
categories: logmon-2
tags: bootcamp
status: Published 
authors: Suraj
Feedback Link: mailto:APAC-SE-Central@dynatrace.com
Analytics Account: UA-175467274-1

# Logs deep dive (2022 edition)
<!-- ------------------------ -->
## Introduction
This repository contains the hands on session for Log Monitoring v2.0.

### Prerequisites
- Dynatrace SaaS/Managed Account. Get your free SaaS trial [here](https://www.dynatrace.com/trial/).
- AWS account, with the ability to create an EC2 instance from a public AMI. Signup to a free trial [here](https://aws.amazon.com/free/).
- Modern web browser
    >- Chromimum based, e.g. Google Chrome, MS Edge
    >- Firefox

### Lab Setup
Following are the details used in this lab.

- [Sock-shop application](https://microservices-demo.github.io/deployment/docker-compose.html) is being run in docker.
- Docker and docker-compose is already setup in the EC2 instance for the participants.
- Logstash, an open-source tool for ETL, is also installed in the user's directory: /home/ec2-user/logstash-6.4.2

### What You'll Learn
- Ingest logs using Logstash
- Transform logs to fit Dynatrace's Log monitoring platform
- How to use the new Log Viewer
- Create and use custom log attributes
- Create and use log metrics
- Create dashlet from custom log metrics
- Create log events

<!-- ------------------------ -->
## Bootcamp Environment Setup

Please claim the EC2 instance from the list. Put your name against the EC2 on the shared excel sheet. The link to the sheet will be provided on the day.

### SSH into your instance

You will be given the .pem (or .ppk) file so you can login to your EC2 instance.

### For Linux users

```
ssh -i ~/.ssh/dt_bootcamp_suraj.pem ec2-user@ec2-54-247-46-254.eu-west-1.compute.amazonaws.com
```

### For Windows users
Use Putty (or similar) to load the "private key file" under the auth section and start a ssh session.

![windows-putty](assets/bootcamp/log2022/putty-windows.jpg)

### Start the Application

You should be logged in as ec2-user.

### Enter the following commands on the cli

This command will start the sock-shop application in docker. 

```
cd /home/ec2-user/microservices-demo
sudo /usr/local/bin/docker-compose -f /home/ec2-user/microservices-demo/deploy/docker-compose/docker-compose.yml up -d
```

The [Sock-shop application](https://microservices-demo.github.io/deployment/docker-compose.html) is being run in docker. During the session we will briefly go through the application architecture.


![The SockShop Application](assets/bootcamp/log2022/sockshop.png)

### Install an environment ActiveGate

In order for us to ingest logs into Dynatrace, we need an active gate. The bootcamp cluster is a managed cluster, so we will be installing an environment AG.

1. Install/Deploy an ActiveGate

>Navigate to the Dynatrace HUB
> - Using the Dynatrace Menu -> Manage -> HUB
> - Search for and click on `ActiveGate`
![log_ag_install](assets/bootcamp/log2022/log_ag_install.png)

Click on `Install ActiveGate`
![log_ag_install2](assets/bootcamp/log2022/log_ag_install_2.png)

>Install ActiveGate
> - Select Linux
> - Click on Generate Token
> - Copy & Execute WGET Command into Linux Server
> - Copy & Execute Install command into Linux Server (*Note: `sudo` is required on this step)
![log_ag_install3](assets/bootcamp/log2022/log_ag_install_3.png)

<!-- ------------------------ -->
## Logs via POST

In this module, we'll:
- Ingest a sample log using Log Ingestion API 

### Log Ingest API
The Log Ingestion API allows for you to push custom logs into Dynatrace. 

1. POST Ingest Logs

> - Replace ADDRESS with the environment AG URL
> - Replace Tenant_ID with valid tenant ID
> - Replace APITOKEN with the value of you API TOKEN
> - Replace HOST_ID with valid host id
> - Replace PG_ID with valid process group id

```
curl -X POST "https://<ADDRESS>:9999/e/<Tenant_ID>/api/v2/logs/ingest" -H "accept: application/json; charset=utf-8" -H "Authorization: Api-Token <APITOKEN>" -H "Content-Type: application/json; charset=utf-8" -d "[{\"content\":\"example log content 1\",\"status\":\"error\",\"log.source\":\"/var/log/syslog\",\"dt.entity.host\":\"<HOST ID>\",\"dt.entity.process_group_instance\":\"<PG_ID>\"},{\"content\":\"example log content 2\",\"status\":\"info\",\"log.source\":\"/var/log/syslog\",\"dt.entity.host\":\"<HOST ID>\",\"dt.entity.process_group_instance\":\"<PG_ID>\"}]" --insecure
```

> - Example LOG JSON

```
[
    {
    "content": "example log content 1",
    "status" : "error",
    "log.source": "/var/log/syslog",
    "dt.entity.host" : "<HOST ID>",
    "dt.entity.process_group_instance": "<PG_ID>"
    "response":12
  },
  {
    "content": "example log content 2",
    "status" : "info",
    "log.source": "/var/log/syslog",
    "dt.entity.host" : "<HOST ID>",
    "dt.entity.process_group_instance": "<PG_ID>",
     "response":1
  }
]
```

![log_ingest_post](assets/bootcamp/log2022/log_ingest_post_example.png)

<!-- ------------------------ -->
## Logs via Logstash - As IS

In your EC2 environment, we have installed logstash 6.4.2 (Apache version 2.0). 
We also have installed the Dynatrace output plugin in the logstash instance for you. For more details, see: [dynatrace-oss/logstash-output-dynatrace](https://github.com/dynatrace-oss/logstash-output-dynatrace)


In this module, we'll:
- Create a Logstash pipeline to read local log files
- Ingest all the logs from the beginning of the file as is
- Understand why we need to transform the ingested logs

### Application logs

First, lets understand whats running in your environment.

```
sudo su
docker ps
```

![docker_ps](assets/bootcamp/log2022/docker_ps.png)

These container logs (by the way they are setup in this env) are going to write logs in the below folder.

```
ls /var/lib/docker/containers/
/var/lib/docker/containers/<container_id>/<container_id>-json.log
```

For instance, the front-end container with ID: `3257c79d012c` will have a logfile "3257c79d012c....... -json.log".

```
cat /var/lib/docker/containers/3257c79d012c1845029c59f07331c87a191bb258306e4c47b091a84877e2c011/3257c79d012c1845029c59f07331c87a191bb258306e4c47b091a84877e2c011-json.log
```

![sample_app_log](assets/bootcamp/log2022/log_front-end.png)

### Create a Logstash Pipeline

Lets now create a config file (a pipeline) for Logstash so that it can read the front-end's log file from the `beginning`.

Steps:

```
sudo su
docker ps |grep front-end
cd /home/ec2-user/logstash-6.4.2/config
vi frontend.conf
```

Paste the below content into the file. Remember to change the `CONTAINER_ID`, `ENV_ACTIVEGATE_HOST`, `TENANT_ID` based on your environment.

(Tip: An easier way to find the file for that container would be to hit tab after you paste the container id after /var/lib/docker/containers/ )

```
input{
  file{
    path => "/var/lib/docker/containers/<FULL_CONTAINER_ID>/<FULL_CONTAINER_ID>-json.log"
    start_position => "beginning"
    codec => json
  }
}

output {
  dynatrace {
    ingest_endpoint_url => "https://<ENV_ACTIVEGATE_HOST>:9999/e/<TENANT_ID>/api/v2/logs/ingest"
    api_key => "<API-KEY>"
    ssl_verify_none => true
  }
}
```
### Execute the pipeline

Test if your pipeline is fine using the below command (note the filename and path may be different each time you run it):

```
/home/ec2-user/logstash-6.4.2/bin/logstash -f /home/ec2-user/logstash-6.4.2/config/frontend.conf -t
```

![test_logstash_pipeline](assets/bootcamp/log2022/test_logstash_pipeline.png)

Execute it using below command:

```
/home/ec2-user/logstash-6.4.2/bin/logstash -f /home/ec2-user/logstash-6.4.2/config/frontend.conf
```

![logstash_running](assets/bootcamp/log2022/execute_logstash_run.png)

### Check the ingested logs in Log Viewer

The logs are ingested as is. 
![sample_app_log](assets/bootcamp/log2022/log_front-end_in_dt.png)

### Repeat the above steps and create another pipeline for catalogue container

Steps:

```
docker ps |grep catalogue
cd /home/ec2-user/logstash-6.4.2/config
cp frontend.conf catalogue.conf
```

Paste the below content into the file. Remember to change the `CONTAINER_ID`, `ENV_ACTIVEGATE_HOST`, `TENANT_ID` based on your environment.

(Tip: An easier way to find the file for that container would be to hit tab after you paste the container id after /var/lib/docker/containers/ )

REPLACE the file path to the log file of the `catalogue` container. 

Execute it using below command: (optionally test it using -t)

```
/home/ec2-user/logstash-6.4.2/bin/logstash -f /home/ec2-user/logstash-6.4.2/config/catalogue.conf
```

The logs are ingested as is. 
![sample_app_log](assets/bootcamp/log2022/log_catalogue_in_dt.png)

<!-- ------------------------ -->
## Custom Log Attribute

In this module, we'll:
- Create custom attributes
- Reingest the application logs
- Filter logs


### Create a custom log attribute

- In the Dynatrace menu, go to Settings.
- Select Log Monitoring > Log custom attributes and then select Add custom attribute.
- Enter the Key as "stream"
- Switch the "Show attribute values in side bar". Save changes
![log_custom_attribute](assets/bootcamp/log2022/log_custom_attribute.png)

You can filter logs based on this new attribute as per below:

![log_attribute_stream_stderr](assets/bootcamp/log2022/log_attribute_stream_stderr.png)

However, the facets will be visible only when new logs with that attribute gets ingested. To emulate this behaviour, we could reingest the same application logs we did in our last step. We just need to delete the `.since_db*` files, which keep tracks of what we have already ingested.


```
sudo su
rm -rf /home/ec2-user/logstash-6.4.2/data/plugins/inputs/file/.sincedb_*
/home/ec2-user/logstash-6.4.2/bin/logstash -f /home/ec2-user/logstash-6.4.2/config/catalogue.conf
```

Once the logs are re-ingested, you will be able to see the attribute "stream" on the facets.

![log_attribute_stream_stderr_facet](assets/bootcamp/log2022/log_attribute_stream_stderr_facet.png)

<!-- ------------------------ -->
## Logs via Logstash - Formatted


In this module, we'll:
- Create transformation in Logstash pipeline to format log files
- Ingest all the logs from the beginning of the file again
- Understand why we need to transform the ingested logs

### Application logs

Lets look at the logs emitted by the catalogue container.

![catalogue_raw_logs](assets/bootcamp/log2022/log_catalogue_raw.png)

You will see a mixed set of log lines. Majority of them are based on key=value pairs. Based on this info, lets construct a "filter" (aka a transformation step) in Logstash.

Steps:

```
cd /home/ec2-user/logstash-6.4.2/config
cp catalogue.conf catalogue_filtered.conf 
vi catalogue_filtered.conf 
```

Enter the below filter declaration in the file between input and output sections:

```
filter { kv { source => "log" } }
```

Execute it using below command: (optionally test it using -t)

```
sudo su
rm -rf /home/ec2-user/logstash-6.4.2/data/plugins/inputs/file/.sincedb_*
/home/ec2-user/logstash-6.4.2/bin/logstash -f /home/ec2-user/logstash-6.4.2/config/catalogue_filtered.conf 
```

Once ingested, you will be able to see the individual key-value pairs as root level attributes.

![log_catalogue_in_dt_filtered](assets/bootcamp/log2022/log_catalogue_in_dt_filtered.png)


We want to extract the number from the attribute `took`. So lets apply one more filter as per below:

```
vi catalogue_filtered.conf 
```

```
  grok {
    match => { "took" => "%{NUMBER:duration}" }
  }
```

Execute the below commands to re-injest the logs.

```
rm -rf /home/ec2-user/logstash-6.4.2/data/plugins/inputs/file/.sincedb_*
/home/ec2-user/logstash-6.4.2/bin/logstash -f /home/ec2-user/logstash-6.4.2/config/catalogue_filtered.conf 
```

The result will look like below:
![log_catalogue_filtered_number](assets/bootcamp/log2022/log_catalogue_filtered_number.png)


Now, lets go and define one additional log attribute: duration.

![log_attribute_duration](assets/bootcamp/log2022/log_attribute_duration.png)

We will use this custom attribute in our next section.

<!-- ------------------------ -->
## Create metric using log viewer

- In the Dynatrace menu, go to Logs.
- Create a log viewer query or use log viewer facets to extract the log data that interests you. 
  ![log_metric_query_filter](assets/bootcamp/log2022/log_metric_query_filter.png)

- Select Create metric. Your log viewer query is now displayed on the Log metrics page.

Adding dimensions allows you to split the log metric occurrences according to specific log data attribute.
Append the metric name to the metric key log. and save changes to create the log metric.

![log_metric_duration](assets/bootcamp/log2022/log_metric_duration.png)


### Using the metric

Now that you have defined the metric, you can chart it, pin it to a dashboard, and even create an alert based on it.

- Chart: Go to Explore data, set Select metricâ€¦ to log.duration, and select Run query.
  ![log_duration_chart](assets/bootcamp/log2022/log_duration_chart.png)

- Dashboard: After you create a chart, select Pin to dashboard to add the chart to one of your dashboards.

- Alert: Go to Settings > Anomaly detection > Custom events for alerting, select Create custom event for alerting, and create a custom event based on log.duration.
  ![log_custom_alert](assets/bootcamp/log2022/log_custom_alert.png)


<!-- ------------------------ -->
## Lets use OneAgent (TM) For Log Collection
- Navigate to Deploy Dynatrace -> Linux
- Generate PaaS token

![install_oneagent_linux](assets/bootcamp/log2022/log_metric_query_filter.png)

Copy the complete URL and API-Tokens. A sample is listed below:

```
wget -O Dynatrace-OneAgent-Linux-1.235.185.sh "https://mou612.managed-sprint.dynalabs.io/e/53d6282c-8893-40b0-ba03-4120b5f10615/api/v1/deployment/installer/agent/unix/default/latest?arch=x86&flavor=default" --header="Authorization: Api-Token ######"
```

Use the URL and API-Tokens obtained to construct the below command:

```
sudo docker run -d --restart=unless-stopped --privileged=true --pid=host --net=host -v /:/mnt/root -e ONEAGENT_INSTALLER_SCRIPT_URL="<PASTE THE URL HERE>" -e ONEAGENT_INSTALLER_DOWNLOAD_TOKEN="<PASTE THE API-TOKEN HERE>" dynatrace/oneagent
```

A sample command would look like below:

```
sudo docker run -d --restart=unless-stopped --privileged=true --pid=host --net=host -v /:/mnt/root -e ONEAGENT_INSTALLER_SCRIPT_URL="https://mou612.managed-sprint.dynalabs.io/e/#####-#####-#####-####-#####/api/v1/deployment/installer/agent/unix/default/latest?arch=x86&flavor=default" -e ONEAGENT_INSTALLER_DOWNLOAD_TOKEN="<PASTE THE API-TOKEN HERE>" dynatrace/oneagent
```

Execute the above command in your EC2 instance.

### Lets explore the log sources found by OneAgent

![log_sources_by_oneagent](assets/bootcamp/log2022/log_sources_by_oneagent.png)

<!-- ------------------------ -->
## Troubleshooting 

### 1. Another instance running
When running Logstash you receive error such as below: 

```
[2022-03-07T06:14:10,345][FATAL][logstash.runner          ] Logstash could not be started because there is already another instance using the configured data directory.  If you wish to run multiple instances, you must change the "path.data" setting.
```

Kill the logstash process that is running. 

### 2. Mistake in pipline/no logs are ingested
If you made a typo/mistake in the pipeline config file and no logs are ingested....

Delete the .since_db file. Those files are under: `/home/ec2-user/logstash-6.4.2/data/plugins/inputs/file/` and named as .sincedb_numbers

![since_db_file](assets/bootcamp/log2022/since_db_path.png)

### 3. 404 erros

Receiving 404 errors during logstash run - Make sure your AG address and api-token are correct.

<!-- ------------------------ -->
## Feedback

We hope you enjoyed this lab and found it useful. We would love your feedback!

Positive
: ðŸ’¡ For other ideas and suggestions, please **[reach out via email](mailto:APAC-SE-Central@dynatrace.com?subject=OpenTelemetry - Ideas and Suggestions")**.